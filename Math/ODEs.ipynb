{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91f371bf-8450-4899-b6fd-60e3effed7d4",
   "metadata": {},
   "source": [
    "### Intuitive Differential Equations\n",
    "\n",
    "#### Description and Goals \n",
    "Here we are going to discuss differential equations and what it means to solve them. The goal here is not really to show mathematical methods to solve them, but to give some intuition about what we are actually doing when we solve a differential equation. The reason we want to do this is because there is a certain point of view we would like to share that is at the core of many mathematical methods in theorectical physics and the treatment of differential equations is one manifestation of this point of view. Methods for solving differential equations will be covered to some extent throughout the course. \n",
    "\n",
    "##### Contributors: Eric Flynn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0d68de-4aad-43cb-9144-aa513f8af574",
   "metadata": {},
   "source": [
    "## Algebraic equations\n",
    "### What does it mean to solve an algebraic?\n",
    "First, we very briefly consider algebraic equations and their solutions. Let's start with a very familar one:\n",
    "\n",
    "$$\\begin{align}\n",
    "a x^{2} + b x + c = 0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Our goal is to find some real or complex numbers $x$ that make the left side of this equation vanish. These special numbers are called \"roots\" or \"solutions\" of the equation. Since this is a polynomial, the fundamental theorem algebra tells us the total number of roots should be equal to the highest power of the polynomial, in this case 2. We know how to solve this equation exactly using the quadratic formula. Exact solutions are just forms of $x$ that can be written down exactly in terms of functions we know. For the quadratic equation it's\n",
    "$$\n",
    "x = \\frac{-b \\pm \\sqrt{b^{2} - 4ac}}{2a}\n",
    "$$\n",
    "with the two solutions indicated by $\\pm$. \n",
    "What about something more complicated? Something like\n",
    "$$ x^{10} + x^{9} + 5x^{4} + x^{2} + x + 9  = 0$$ \n",
    "Here, the goal is the same but we no longer have a formula to solve the equation. It's a polynomial so we know there should be 10 solutions but we don't have a closed form expression of the roots. The point being, even though we can't solve this in closed form, we still know something about their solutions, namely there is exactly 10 real or complex numbers that satisfy the equation.\n",
    "We can even have some terror like this:\n",
    "$$ \\tan\\Big({\\frac{ax}{2}}\\Big) = x $$ \n",
    "\n",
    "This is an example of a \"transendental equation\" where, again we are looking for all $x$ that satisfies the equal sign but here we can have infinitely many solutions based on properties of the $\\tan$ function. We also don't know a closed form solution to this equation. This kind of equation results from the finite square well problem in quantum mechanics you may have seen before. Typically, if the problem is easy enough, one can use approximation methods like perturbation theory or asymptotic methods when closed form solutions cannot be found. In practice however, numerical methods are often used to get fast and accurate roots. \n",
    "\n",
    "This is essentially the whole practical story for algebraic equations. We emphasize that the \"solution\" to these equations are numbers. The question is, do there exist equations where \"roots\" are other objects like functions? The answer is yes and this precisely the context of the theory of differential equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71992898-edb7-4c55-9f5b-a8e3002ec66b",
   "metadata": {},
   "source": [
    "# Linear equations and their roots.\n",
    "Let's take a step back and consider just linear equations. From linear algebra, we learned how to solve coupled linear equations. For example, lets take the system\n",
    "$$\n",
    "\\begin{align}\n",
    "a_{00}v_{0} + a_{01} v_{1} + a_{02} v_{2}  &= b_{0} \\\\\n",
    "a_{10}v_{0} + a_{11} v_{1} + a_{02} v_{2}  &= b_{1} \\\\\n",
    "a_{20}v_{0} + a_{21} v_{1} + a_{22} v_{2}  &= b_{2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "This is a $3 \\times 3$ system of linear equations where, if we specify what $a_{ij}$ and $b_{i}$ are, our roots or solutions are some numbers $v_{0},v_{1},v_{2}$. But this is just a matrix equation that we have seen in linear algebra. We can instead more compactly write this as \n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "a_{00} & a_{01} & a_{02} \\\\\n",
    "a_{10} & a_{11} & a_{12} \\\\\n",
    "a_{20} & a_{21} & a_{22} \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "v_{0} \\\\\n",
    "v_{1} \\\\\n",
    "v_{2} \n",
    "\\end{pmatrix}\n",
    "=\\begin{pmatrix}\n",
    "b_{0} \\\\\n",
    "b_{1} \\\\\n",
    "b_{2} \n",
    "\\end{pmatrix}\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "$$\n",
    "A\\vec{v} = \\vec{b}\n",
    "$$\n",
    "The vector $\\vec{v}$ and $\\vec{b}$ are always represented with respect to a basis. These vectors in particular are typically represented with repect to the standard basis:\n",
    "$$\n",
    "\\begin{align}\n",
    "e_{0} = \n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{pmatrix}, \\hspace{3mm}\n",
    "e_{1} = \n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix}, \\hspace{3mm}\n",
    "e_{2} = \n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{v} = \\begin{pmatrix}\n",
    "v_{0} \\\\\n",
    "v_{1} \\\\\n",
    "v_{2}\n",
    "\\end{pmatrix} = v_{0}\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{pmatrix} + v_{1}\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix} + v_{2}\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix} = v_{0}e_{0} + v_{1} e_{1} + v_{2} e_{2}\n",
    "\\end{align}\n",
    "$$\n",
    "and similar for $\\vec{b}$. It turns out, the basis of $\\vec{v}$ doesn't necessarily need to be a set of vectors, they can also be functions. For example, we can take \n",
    "$$e_{0} = 1, \\hspace{3mm} e_{1} = x, \\hspace{3mm} e_{2} = x^{2}$$ \n",
    "as a basis. This is a orthogonal basis (you can check this by computing the dot product $e_{i}\\cdot e_{j}$ over a finite interval). If we let $x$ be a real variable, then this is a valid choice of basis on a finite interval since the set of all polynomials form a vector space. All this essentially means is any linear combination of polynomials is another polynomial and contains a zero vector. If we choose polynomials to be our basis, our \"vector\" $\\vec{v}$ now looks like \n",
    "$$\n",
    "\\vec{v} = v_{0} + v_{1}x + v_{2} x^{2}\n",
    "$$\n",
    "This is now a polynomial in variable $x$. But we still have all the same machinary from linear algebra: all we did is just call $\\vec{e}_{i}$ a function of $x$ and the matrix equation we started with earlier still has meaning.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "a_{00} & a_{01} & a_{02} \\\\\n",
    "a_{10} & a_{11} & a_{12} \\\\\n",
    "a_{20} & a_{21} & a_{22} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "v_{0} \\\\\n",
    "v_{1} \\\\\n",
    "v_{2} \\\\\n",
    "\\end{pmatrix} \n",
    "=\\begin{pmatrix}\n",
    "b_{0} \\\\\n",
    "b_{1} \\\\\n",
    "b_{2} \n",
    "\\end{pmatrix}\n",
    "\\end{align} \n",
    "$$\n",
    "where now it is relative to the polynomial basis.\n",
    "So now the question is, what combination of $1, x, x^{2}$ can satisfy this linear equation? This depends on what $A$ and $b$ is. One of the most common operations we can impose on a function is differentiation. Starting with $\\vec{v}$ again,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{v} = f(x) = v_{0} + v_{1}x + v_{2} x^{2}\n",
    "\\end{align}\n",
    "$$\n",
    "we can take a derivative,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dx} f(x) = v_{1} + 2 v_{2} x\n",
    "\\end{align}\n",
    "$$\n",
    "Note we took a derivative of $f(x)$ and we got back another polynomaial that is 1 degree lower. In this picture, the derivative can be represented as a matrix equation\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 2 \\\\\n",
    "0 & 0 & 0 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "v_{0} \\\\ \n",
    "v_{1} \\\\\n",
    "v_{2}\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "v_{1}\\\\\n",
    "2v_{2} \\\\\n",
    "0 \n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "Note that the matrix $A$ is not invertible and this system will have either infinitly many solutions or none at all. If we want solutions or \"roots\" of $A\\vec{v} = 0$, we must have a root $\\vec{v}^{*}$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{v}^{*} =\n",
    "\\begin{pmatrix}\n",
    "v_{0} \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{pmatrix} = v_{0} e_{0} = v_{0}\n",
    "\\end{align} \n",
    "$$\n",
    "But this is exactly the general solution to the differential equation \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{df}{dx} = 0 \n",
    "\\end{align}\n",
    "$$\n",
    "The \"roots\" of differential operators are functions of $x$ in the polynomial basis. We just solved a differential equation by choosing a basis of $\\{1,x,x^{2}\\}$, and focusing our attention on how derivatives act on vectors represented in this basis. Once we had a matrix representation, we used our machinary from linear algebra to find \"roots\" of the differential operator represented in this basis. Granted, we just solved the simplest possible differential equation but this analysis hold for higher order polynomaials.\n",
    "\n",
    "Some comment about boundary conditions or initial conditions are in order. In this simple case, the general solution we found is representing infinitely many possible solutions because $v_{0}$ is arbitrary. This was expected since $A$ was not invertible. We can fix a particular solution by enforcing a boundary or initial condition. For example, we can force $f(x)$ to be one number at $x=0$. \n",
    "$$\n",
    "\\begin{align}\n",
    "f(x=0) = 1\n",
    "\\end{align}\n",
    "$$\n",
    "which fixes $v_{0} = 1$ and $f(x) = 1$. Now we have a single unique solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe193d-176a-4d17-96ac-01ae3f9e1225",
   "metadata": {},
   "source": [
    "# Taking things further: Adding more dimensions\n",
    "Now lets suppose we have a \"vector\" $\\vec{v}$ expressed in the polynomial basis again, \n",
    "$$\n",
    "\\vec{v} = \\sum_{n= 0}^{N} v_{n} \\vec{e}_{n} = \\sum_{n= 0}^{N} v_{n} x^{n} = f(x)\n",
    "$$ \n",
    "How does the derivative operator now operate on this? \n",
    "$$\n",
    "\\frac{d}{dx}f(x) = \\sum_{n=0}^{N} v_{n} n x^{n-1} = \\sum_{n=1}^{N} v_{n} n x^{n-1}\n",
    "$$\n",
    "The derivative matrix has the same structure as before:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dx} = \n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 2 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & 3 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 4 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 5 & \\dots & 0 \\\\\n",
    "\\vdots &\\vdots  & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & \\dots & N  \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "The derivative is now a big $N \\times N $ matrix and operates on our vector $\\vec{v}$,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dx} f(x) = \n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 2 & 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & 3 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 4 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 5 & \\dots & 0 \\\\\n",
    "\\vdots &\\vdots  & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & \\dots & N  \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "v_{0} \\\\\n",
    "v_{1} \\\\\n",
    "v_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\vdots \\\\\n",
    "v_{N-1} \\\\\n",
    "v_{N}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "= \\sum_{n=1}^{N} v_{n} n x^{n-1}\n",
    "$$\n",
    "Now take $N \\rightarrow \\infty$\n",
    "$$\n",
    "f(x) =  \\sum_{n=0}^{\\infty} v_{n} x^{n}\n",
    "$$\n",
    "Our \"vector\" has now become a Taylor series for a function $f(x)$. So this shows we can view a function as an infinite dimensional vector. Note we made the assumption that the function $f(x)$ we want to represent in the polynomial basis is \"analytic\". This means the series representing $f(x)$ exists for all $x$. Not all functions will be analytic but the story of non-analytic functions is not relevent for our discussions here. The differential operator then becomes an infinite dimensional matrix acting on an infinite dimensional vector: \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dx} f(x) = \n",
    "\\begin{pmatrix}\n",
    "0 & 1 & 0 & 0 & 0 & 0 & \\dots \\\\\n",
    "0 & 0 & 2 & 0 & 0 & 0 & \\dots \\\\\n",
    "0 & 0 & 0 & 3 & 0 & 0 & \\dots \\\\\n",
    "0 & 0 & 0 & 0 & 4 & 0 & \\dots \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 5 & \\dots \\\\\n",
    "\\vdots &\\vdots  & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "v_{0} \\\\\n",
    "v_{1} \\\\\n",
    "v_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\vdots \\\\\n",
    "\\vdots \n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "By analogy with the finite dimensional case, this shows that the \"roots\" of a differential operator are formally functions $f(x)$ because if we solve an infinite dimensional linear system, we can define every coeffecient in the Taylor series representing $f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ecd62-5641-4092-9f88-50c5d50bf69c",
   "metadata": {},
   "source": [
    "# Adding more stucture: Linear differential equations\n",
    "In the previous section we showed that solutions to linear differential equations of the form \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{df}{dx} = g(x)\n",
    "\\end{align}\n",
    "$$\n",
    "can intuitively viewed as finding roots of an infinte dimensional matrix equation $\\frac{d}{dx} f = g(x)$ under some conditions of analyticity. Lets consider a more complicated class of differential equations: \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d^{2}f}{dx^{2}} + a \\frac{df}{dx} + bf = 0 \n",
    "\\end{align}\n",
    "$$\n",
    "or \n",
    "$$\n",
    "\\begin{align}\n",
    "\\Big(\\frac{d^{2}}{dx^{2}} + a \\frac{d}{dx} + b\\Big)f = \\hat{L}f = 0 \n",
    "\\end{align}\n",
    "$$\n",
    "where $a,b$ are constant real or complex numbers. To solve this, we consider the ansatz $f(x) = e^{r x}$ where $r$ is a real or complex number. Plugging this into the ODE we get  \n",
    "$$\n",
    "\\begin{align}\n",
    "(r^{2} + ar + b)f(x) = 0 \n",
    "\\end{align}\n",
    "$$\n",
    "which is true if and only if\n",
    "$$\n",
    "\\begin{align}\n",
    "r^{2} + ar + b = 0 \n",
    "\\end{align}\n",
    "$$\n",
    "So we have reduced the problem of finding a function $f(x)$ to a simple algebra problem where we need to find roots of an algebraic equation. Let $r_{0},r_{1}$ be a root of this above quadratic equation, then we have a factorization, \n",
    "$$\n",
    "\\begin{align}\n",
    "(r - r_{0})(r- r_{1})  = 0 \n",
    "\\end{align}\n",
    "$$\n",
    "Since we have two algebraic roots, we must have two \"roots\" of the differential operator $\\hat{L}$. The general solution is \n",
    "$$\n",
    "\\begin{align}\n",
    "f(x) = c_{0} e^{r_{0} x} + c_{1} e^{r_{1} x} \n",
    "\\end{align}\n",
    "$$\n",
    "This implies the following factorization of the differential operator $\\hat{L}$,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Big(\\frac{d}{dx} - r_{0}\\Big)\\Big(\\frac{d}{dx}- r_{1}\\Big)f(x) = 0 \n",
    "\\end{align}\n",
    "$$ \n",
    "What this shows is any differential operator with constant coefficient can always be factored just like a polynomial. Because of this fact, the fundamental theorem of algebra also applies and we know the second-order differential equation we are considering has two indepedent functions as solutions (to be more precise there are theorems about existence and uniqueness of solutions but we don't go into that here).\n",
    "\n",
    "### Non-constant coefficients \n",
    "\n",
    "So we basically solved all possible ODEs that have constant coefficents. What about a differential equation that does not have constant coefficents? Does the same trick work? The short answer is no but it is interesting to consider why it doesn't work. Consider the following differential equation as an example:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Big(\\frac{d^{2}}{d x^{2}} + x\\Big) f = \\hat{L}f = 0 \n",
    "\\end{align}\n",
    "$$\n",
    "Let $\\frac{d}{dx} = D$. If $\\hat{L}$ can be factored then, we look for an operator of the form $\\Big(D - g_{1}(x)\\Big)\\Big(D - g_{2}(x)\\Big)$ such that \n",
    "$$\n",
    "\\frac{d^{2}}{d x^{2}} + x = \\Big(D - g_{1}(x)\\Big)\\Big(D - g_{2}(x)\\Big)\n",
    "$$\n",
    "But we can also have the factorization \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d^{2}}{d x^{2}} + x = \\Big(D - g_{2}(x)\\Big)\\Big(D - g_{1}(x)\\Big)\n",
    "\\end{align}\n",
    "$$\n",
    "Expanding both out,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Big(D - g_{1}(x)\\Big)\\Big(D - g_{2}(x)\\Big) = D^{2} - D g_{2}(x) - g_{1}(x)D + g_{1}(x)g_{2}(x) \n",
    "\\end{align}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\begin{align}\n",
    "\\Big(D - g_{2}(x)\\Big)\\Big(D - g_{1}(x)\\Big) = D^{2} - D g_{1}(x) - g_{2}(x)D + g_{1}(x)g_{2}(x) \n",
    "\\end{align}\n",
    "$$\n",
    "We notice these two operators are very different because \n",
    "$$\n",
    "\\begin{align}\n",
    "g_{1}(x)D \\neq D g_{1}(x), \\hspace{3mm} g_{2}(x)D \\neq D g_{2}(x) \n",
    "\\end{align}\n",
    "$$\n",
    "The polynomial operators no longer commute! This means a factorization like we found for constant coeffcient equations is no longer well defined, there's no obvious way to know what order the factors should be in. Other methods will need to be used instead. In fact, one trick that works sometimes is the same one we used in previous sections, that is, if we represent $f(x)$ as a Taylor series, and attempt to solve for the coeffcients of the series again, we can sometimes solve non-constant coeffcient ODES. This is exactly the \"series solution\" method or \"method of Frobenius\" that may have popped up in a math methods or quantum mechanics course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cc36b9-0088-44ee-bb88-1d57c963d7f0",
   "metadata": {},
   "source": [
    "# Eigenvalues and Eigenvectors Again\n",
    "\n",
    "In the previous section, we used an ansatz $f(x) = e^{rx}$ to solve the constant coeffecient ODEs. And maybe you have also noticed that exponential functions alway seem to pop up when solving other differential equations. Why? There is one motivation. Take a derivative of $ f(x) = e^{\\lambda x}$ where $\\lambda$ is some real or complex number.\n",
    "$$\n",
    "\\frac{df}{dx}= \\frac{d}{dx} e^{\\lambda x} = \\lambda e^{\\lambda x} = \\lambda f, \\hspace{3mm} \\longrightarrow \\hspace{3mm} \\frac{df}{dx} = \\lambda f\n",
    "$$\n",
    "The exponential is an eigenfunction of the derivative operator with eigenvalue $\\lambda$. This makes exponentials very good guesses for many differential equations. But this was just a single derivative. The next step up is \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d^{2}f}{dx^{2}}= -\\lambda^{2} f(x) \n",
    "\\end{align}\n",
    "$$\n",
    "Note we changed $\\lambda$ to $- \\lambda^{2}$. This is just for convienence for what we are going to do next (it just reduces the amount of algebra you need to do). The general solution here is again \n",
    "$$\n",
    "\\begin{align}\n",
    "f(x) = c_{0} e^{i \\lambda x} + c_{1} e^{-i \\lambda x}\n",
    "\\end{align}\n",
    "$$\n",
    "or equivalently, \n",
    "$$\n",
    "\\begin{align}\n",
    "f(x) = c_{0} \\cos(\\lambda x) + c_{1} \\sin(\\lambda x)\n",
    "\\end{align}\n",
    "$$\n",
    "This is as far as we can go with a general solution and quite frankly this is not a useful solution at all because we have 3 unknowns: $c_{0},c_{1},\\lambda$. To get any use out of these, we need 3 equations that will allow us to find $c_{0},c_{1},\\lambda$. These will be supplied by boundary conditions and a normalization.\n",
    "Let's enforce $f(x = 0) = 0$ and $f(x = L) = 0$\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x = 0) &= c_{0} = 0 \\\\\n",
    "f(x = L ) &= c_{0}\\cos(\\lambda L) + c_{1}\\sin(\\lambda L)  = 0\n",
    "\\end{align}\n",
    "$$\n",
    "This leads to \n",
    "$$\n",
    "\\sin(\\lambda L) = 0 \n",
    "$$\n",
    "which is only true for specific values of $\\lambda$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\lambda_{n} = \\frac{n \\pi}{L}, \\hspace{5mm} n = 0, 1, 2, ...\n",
    "\\end{align}\n",
    "$$. \n",
    "The boundary conditions gave us a restriction on $\\lambda$ (a quantization!) and we have a full family of solutions \n",
    "$$\n",
    "\\begin{align}\n",
    "f_{n}(x) = \\sqrt{\\frac{2}{L}} \\sin(\\lambda_{n} x) \n",
    "\\end{align}\n",
    "$$\n",
    "for every positive integer $n$ where $c_{1}$ was fixed by noramlization \n",
    "$$\n",
    "\\int_{0}^{L} |f(x)|^{2} = 1 \n",
    "$$\n",
    "The set of eigenvectors are all orthonormal on the interval $[ 0, L]$. This is basically a particle in an infinite square well and the solutions are just \"standing waves\", $\\sin(\\lambda_{n} x)$. This is interesting since these solutions are also found in classical mechanics where we might have a vibrating string in 1d with the ends of the strings tied. \n",
    "\n",
    "There is one more interesting thing about this story. One might wonder if we can construct a basis for a vector space using the basis set $\\{\\sin(\\lambda_{n} x)\\}$ in the same way as we used polynomials. Well we can and it takes a familar form, \n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{v} = \\sum_{n=0}^{\\infty} v_{n} \\sin(\\lambda_{n} x) \n",
    "\\end{align}\n",
    "$$\n",
    "This is just an odd Fourier series. So from this point of view, an odd fourier series is just a linear combination of eigenvectors of the infinite square well eigenvalue problem defined above. Turns out this set is \"complete\" and span the space of odd periodic functions.\n",
    "\n",
    "Now this brings us to something that's fundamental to quite a lot of physics, mathematics, and computation. The whole reason we spend so much time trying to solve the time-independent Schrodinger equation is because we believe the eigenfunctions we hope to find are a good basis for the Hilbert space of our quantum mechanics problem. The more technical jargon for what we are doing is we hope to find a basis for the space $L^{2}$ which stands for \"Lebesgue space\" of degree 2; the space of square integrable functions (i.e functions that satisfy the relation \n",
    "$$\n",
    "\\begin{align}\n",
    "\\int f^{*}(x) f(x) dx = 1 \n",
    "\\end{align}\n",
    "$$.)\n",
    "This $L^{2}$ is precisely the space spanned by solutions to the time-independent schrodinger equation. The general theory for this particular problem is called Strum-Liouville Theory. This is what we do when we solve the Schrodinger equation and is more generally at the heart of many problems in physics, computational physics and mathematics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50363a6e-3188-44c1-84e1-0bd9a98d9970",
   "metadata": {},
   "source": [
    "# Nonlinear Differential Equations\n",
    "For the very last topic, let's have a very quick look at non-linear differential equations since these are the types of equations that you will probably encounter in research. The algebraic analog for these differential equations are the transendental equations we saw in section 1. Numerical solutions to these differential equations are really your only practical way of solving non-linear differential equations and even then, they are very formidable. But, you can STILL leverage the same machinary we have discussed in this module to try to attack very difficult non-linear differential equations you wouldn't have a chance to solve by hand. \n",
    "\n",
    "In the previous section we saw we can (hopefully) find functions that are solutions to Strum-Liouville problems. The set of eigenfunctions form a complete basis for $L^{2}$. Since the solutions form a basis for such a general space, one might hope that it will also be a good basis for another differential operator. Turns out this is a very powerful way of thinking about solutions to horrible differential equations. Let's just look at an example. Consider some miserable differential equation like \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d^{2}\\phi}{d x^{2}} + \\phi^{3} + x^{2}\\phi = 0  \n",
    "\\end{align}\n",
    "$$\n",
    "with boundary conditions $\\phi(x = \\pm \\infty) = 0 $. Depending on the nature of the problem, one can choose to expand $\\phi$ in some set of known functions resulting from a Strum-Liouville problem. For example, one could choose quantum harmonic oscillator states (you'll see these more detail later in the course):\n",
    "\n",
    "$$\n",
    "\\psi_{n}(x) = \\Big(\\frac{m \\omega}{\\pi \\hbar}\\Big)^{1/4} \\frac{1}{\\sqrt{2^{n} n!}} H_{n}\\Big(\\sqrt{\\frac{m \\omega}{2\\hbar}} x\\Big)e^{- \\frac{m \\omega}{\\hbar} x^{2}}\n",
    "$$\n",
    "\n",
    "Given our boundary conditions, we might expect harmonic oscillator functions to be a good choice since they automatically satisfy the boundary conditons. So to solve the non-linear equation, we make the ansatz \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\phi(x) = \\sum_{n = 0}^{N} c_{n} \\psi_{n}(x)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "From there, you can derive how the derivative operators act on $\\phi$ just we did for the polynomial basis and find a matrix representation of derivative in the harmonic oscillator basis. Once you have a matrix representation, you can start using numerical methods to solve your system of equations for $c_{n}$. This approach to solving non-linear equations goes by many names and some of the most common ones are \"collocation method\", \"spectral method\", \"pseudo-spectral method\", and discrete variable representation (DVR)\". There are some slight technical variations between these methods but they all rooted in the same basic idea.\n",
    "\n",
    "This is a massive field of applied mathematics and in case you are interested, following are some good references on this:\n",
    "\n",
    "1) Boyd, John P. Chebyshev and Fourier spectral methods. Courier Corporation, 2001.\n",
    "2) Trefethen, Lloyd N. Spectral methods in MATLAB. Society for industrial and applied mathematics, 2000.\n",
    "3) Press, William H., William T. Vetterling, Saul A. Teukolsky, and Brian P. Flannery. Numerical recipes. Cambridge University Press, London, England, 1988."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c2035-100a-4c71-b46a-ece5503ecc8c",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
