{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "- identify discrepancy between ```autograd``` and regular implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will focus on the concepts and fundamentals of $\\chi^2$ minimization. For an in-depth review see [J Dobaczewski et al 2014 J. Phys. G: Nucl. Part. Phys. 41 074001](https://iopscience.iop.org/article/10.1088/0954-3899/41/7/074001).\n",
    "\n",
    "$\\chi^2$ minimization is a specific case of so-called optimization problems. These problems focus on using some convex function to force a minimum to appear in the function itself, thereby allowing us to identify important features from the parameters of the function. In other words, optimization problems use a special set of functions like $\\chi^2$ (shown below) to generate a dimensionless quantity that can be used to identify important aspects between theoretical models and experimental data. This is reflected in the $\\chi^2$ function\n",
    "$$\n",
    "\\chi^2(\\vec{p}) = \\sum_{i=1}^{N_d} \\frac{\\left(O_i(\\vec{p}) - O_{i}^{\\rm exp}\\right)^2}{\\Delta O_i^2}\n",
    "$$\n",
    "where $\\vec{p}$ is a vector containing the paremeters, e.g. $\\vec{p}=(p_0, p_1,...,p_n)$ for distinct parameters $p_i$, $O_i(\\vec{p})$ is the $i$-th model or theoretically calculated value, $O_i^{\\rm exp}$ is the $i$-th  experimental data point, and $\\Delta O_i$ is the $i$-th adopted error.\n",
    "\n",
    "Generally these (penalty) functions can be chosen or tailored to your specific problem. In many cases, using $\\chi^2$ is a decent starting point. We will elaborate more on the innerworkings of the process as we go, but there are a few general ideas to remember as we go through\n",
    "\n",
    "1. What type of data do you have? In some cases, it might be better to consider different forms of the data, like taking a log for data which varies by orders of magnitiude.\n",
    "\n",
    "2. The error term $\\Delta O_i$ is not *only* the experimental error, but a sum of $\\Delta O_i^2 = (\\Delta O_i^{\\rm exp})^2 + (\\Delta O_i^{\\rm num})^2 + (\\Delta O_i^{\\rm the})^2$ for the experimental, numerical, and theoretical errors respectively.\n",
    "\n",
    "3. When we refer to \"residuals\" these are the differences between experimental and calculated theoretical value.\n",
    "\n",
    "4. By using a dimensionless function like $\\chi^2$, we can use a variety of observables for each datapoint $i$ as we will calculate a difference between theory and experimental value $\\left(O_i(\\vec{p}) - O_{i}^{\\rm exp}\\right)^2$ divided by the same units from the error $\\Delta O_i^2$ resulting in a cancelation of units.\n",
    "\n",
    "5. The error $\\Delta O_i^2$ can be somewhat arbitrary due to the dependence on user input. Regardless, you can consider this as a weight term by $W_i = 1 / \\sqrt{\\Delta O_i}$.\n",
    "\n",
    "Before we get started, we'll import some (hopefully) familiar packages that we'll use a lot later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the dataset\n",
    "\n",
    "We assume that you are following along with or participating in the Nuclear Structure Course (PHY981) offered at Michigan State University or a similar course at your current institution. If you are not, a previous assignment titled \"introduction_to_reading_data_and_plotting.ipynb\" focused on generating the data set we will be using. We recommend you take a look at that notebook now or at minimum the \"Assignment\" section.\n",
    "\n",
    "Now, we can read the data from the file we created in the previous assignment (should be a file named ```assignment_output_for_reading_data_and_plotting.csv```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('assignment_output_for_reading_data_and_plotting.csv')\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our data now using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same code as before...\n",
    "fig, ax = plt.subplots(figsize=[5,5]) # Figure and axis subplot panel respectively\n",
    "\n",
    "# We will plot data directly onto the axis panel\n",
    "ax.errorbar(\n",
    "    dataset['x'], dataset['y'], yerr=2*dataset['error'], # plot x, y, and error data\n",
    "    fmt='o', capsize=7, # set the format to be circular data points and error bar width size\n",
    "    label='Data' # Note that we now include a label to go with our legend later\n",
    ")\n",
    "\n",
    "# Set the x and y-labels for the plot\n",
    "ax.set_xlabel('x',fontsize=16)\n",
    "ax.set_ylabel('y',fontsize=16)\n",
    "\n",
    "# Set x and y limits\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-2,4)\n",
    "\n",
    "# Set title\n",
    "ax.set_title('Example data plot', fontsize=16)\n",
    "\n",
    "# Include a legend on the given axis to the top left quadrant\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Calculate $\\chi^2$ minimization\n",
    "\n",
    "In the following we will create our $\\chi^2$ python function and use the package ```scipy``` which has a lot of useful functions like ```optimize.minimize``` to minimize a given function. Before we go any further, we'll make sure this is setup using the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the model\n",
    "\n",
    "In the introduction, we hinted at a key idea which we will expand upon now. All of this minimization discussion is centered around a model we pick. Let's imagine we don't know the form of the random data above and are left to come up with our own model. It seems to have $y$ increase proportionally with $x$ so perhaps a good model for this would be a linear one like\n",
    "$$\n",
    "y = mx + b.\n",
    "$$\n",
    "Of course, if we knew something about the data, maybe we would decide a linear one is not the best and pick another. For now, we'll assume we have no underlying knowledge about the data form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(x, m, b):\n",
    "    '''\n",
    "    Function returning the calculated value of a simple line. Given slope m and intercept b, calculates and returns the linear value y.\n",
    "    '''\n",
    "    return m * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function, we will give an example of its use in relation to the data. Since we've picked a model based on a presumed form of the data (that is seems linear), we might want to see if this holds up. For illustration sake, we pick a random set of parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial parameters for m and b respectively\n",
    "initial_parameters = [2, -0.25]\n",
    "\n",
    "# Calculate the value for our model given the initial parameters\n",
    "modeled_y = linear_model(dataset['x'], initial_parameters[0], initial_parameters[1])\n",
    "\n",
    "# Use the same code as in the \"introduction_to_reading_data_and_plotting.ipynb\" notebook...\n",
    "fig, ax = plt.subplots(figsize=[5,5]) # Figure and axis subplot panel respectively\n",
    "\n",
    "# We will plot data directly onto the axis panel\n",
    "ax.errorbar(\n",
    "    dataset['x'], dataset['y'], yerr=2*dataset['error'], # plot x, y, and error data\n",
    "    fmt='o', capsize=7, # set the format to be circular data points and error bar width size\n",
    "    label='Data' # Note that we now include a label to go with our legend later\n",
    ")\n",
    "\n",
    "# Now we can create any arbitrary line and plot it\n",
    "ax.plot(dataset['x'], modeled_y, label='Initial Guess')\n",
    "\n",
    "# Set the x and y-labels for the plot\n",
    "ax.set_xlabel('x',fontsize=16)\n",
    "ax.set_ylabel('y',fontsize=16)\n",
    "\n",
    "# Set x and y limits\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-2,4)\n",
    "\n",
    "# Set title\n",
    "ax.set_title('Example data plot', fontsize=16)\n",
    "\n",
    "# Include a legend on the given axis to the top left quadrant\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a quick glance, this doesn't look too bad but it certianly isn't a systematic or rigerous way to pick the parameters. Let's now do our procedure to get the optimized parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our $\\chi^2$ function\n",
    "\n",
    "We will define our $\\chi^2$ python function below and aim to keep it as general as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for linear chi^2\n",
    "def linear_chi_square(parameters, data_frame, theoretical_error_type):\n",
    "    m, b = parameters\n",
    "    \n",
    "    # We will get the column data as numpy arrays to more easily handle the data\n",
    "    experimental_x = data_frame['x'].to_numpy()\n",
    "    experimental_y = data_frame['y'].to_numpy()\n",
    "    experimental_error = data_frame['error'].to_numpy()\n",
    "\n",
    "    # Calculate the theoretical values\n",
    "    theory_y = linear_model(experimental_x, m, b)\n",
    "\n",
    "    # Calculate the residuals\n",
    "    residuals = experimental_y - theory_y\n",
    "\n",
    "    # Calculate the total errors i.e. O^{exp} + O^{num} + O^{the}\n",
    "    # Depending on our preference, we can either calculate using no theoretical error or a simple RMSE\n",
    "    if theoretical_error_type.lower() == 'none':\n",
    "        theoretical_error = 0\n",
    "    elif theoretical_error_type.lower() == 'rmse':\n",
    "        theoretical_error = np.sqrt(np.mean(residuals**2)) # Use RMSE to estimate theoretical error\n",
    "    \n",
    "    # calculate total error\n",
    "    total_error = experimental_error + theoretical_error\n",
    "\n",
    "    # Calculate the chi^2\n",
    "    chi_square = (residuals**2) / (total_error**2)\n",
    "\n",
    "    # Return sum over all chi^2 values\n",
    "    return np.sum(chi_square)\n",
    "\n",
    "# Create a function which will also calculate the normalized linear function chi^2\n",
    "def normalized_linear_chi_square(parameters, data_frame, theoretical_error_type):\n",
    "    chi_square = linear_chi_square(parameters, data_frame, theoretical_error_type)\n",
    "    n_parameters = len(parameters)\n",
    "    n_data = len(data_frame['y'])\n",
    "    return chi_square / (n_data - n_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our $\\chi^2$ function, we can minimize it given our initial parameter guess. Note in the above function, we assumed the form of the total error\n",
    "$$\n",
    "\\Delta O_i^2 = (\\Delta O_i^{\\rm exp})^2 + (\\Delta O_i^{\\rm num})^2 + (\\Delta O_i^{\\rm the})^2\n",
    "$$\n",
    "with\n",
    "\\begin{align}\n",
    "\\Delta O_i^{\\rm exp} &= {\\rm experimental\\; values} \\nonumber \\\\\n",
    "\\Delta O_i^{\\rm num} &= 0 \\nonumber \\\\\n",
    "\\Delta O_i^{\\rm the} &= \\sqrt{\\frac{1}{N}\\sum_{i}^{N}(O_i(\\vec{p}) - O_{i}^{\\rm exp})^2}. \\nonumber \n",
    "\\end{align}\n",
    "The choice of theoretical uncertainties can be modified if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimize $\\chi^2$\n",
    "\n",
    "Now that we have our $\\chi^2$ python function, we can use ```scipy.optimize.minimize``` to minimize it given some initial conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform minimization\n",
    "minimized_result = minimize(linear_chi_square, initial_parameters, args=(dataset, 'RMSE'))\n",
    "\n",
    "# Let's look at the optimized parameters\n",
    "optimized_parameters = minimized_result.x\n",
    "\n",
    "# We will print only up to the first 3 decimals\n",
    "print('Optimized m = {:.3f}'.format(optimized_parameters[0]))\n",
    "print('Optimized b = {:.3f}'.format(optimized_parameters[1]))\n",
    "\n",
    "# We can also print the minimized chi^2 value\n",
    "minimum_chi_square = minimized_result.fun\n",
    "\n",
    "print(\"Minimum chi^2 value = {:.3f}\".format(minimum_chi_square))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at our minimized result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an analytical x domain beyond our dataset just so we can get a feel for how it may extrapolate\n",
    "x = np.linspace(0, 1.5, 100)\n",
    "\n",
    "# Calculate the value for our model given the initial parameters\n",
    "modeled_y = linear_model(x, optimized_parameters[0], optimized_parameters[1])\n",
    "\n",
    "# Use the same code as in the \"introduction_to_reading_data_and_plotting.ipynb\" notebook...\n",
    "fig, ax = plt.subplots(figsize=[5,5]) # Figure and axis subplot panel respectively\n",
    "\n",
    "# We will plot data directly onto the axis panel\n",
    "ax.errorbar(\n",
    "    dataset['x'], dataset['y'], yerr=2*dataset['error'], # plot x, y, and error data\n",
    "    fmt='o', capsize=7, # set the format to be circular data points and error bar width size\n",
    "    label='Data' # Note that we now include a label to go with our legend later\n",
    ")\n",
    "\n",
    "# Now we can create any arbitrary line and plot it\n",
    "ax.plot(x, modeled_y, label='Optimized Parameters')\n",
    "\n",
    "# Set the x and y-labels for the plot\n",
    "ax.set_xlabel('x',fontsize=16)\n",
    "ax.set_ylabel('y',fontsize=16)\n",
    "\n",
    "# Set x and y limits\n",
    "# ax.set_xlim(0, 1)\n",
    "ax.set_ylim(-2,4)\n",
    "\n",
    "# Set title\n",
    "ax.set_title('Example data plot', fontsize=16)\n",
    "\n",
    "# Include a legend on the given axis to the top left quadrant\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, this doesn't look much more impressive compared to our original guess, so why bother with a process like this? Well, the point of minimization isn't only to get a nice fitting line to our data, but to also be able to say something more concrete about why its a good calibration! We can also see how the $\\chi^2$ space changes with varying values of $m$ and $b$ in the 3D plot below. This is just to highlight the minimum, and statistical considerations will be elaborated on more below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mesh grid and parameter range for m and b\n",
    "m_range = np.linspace(-5, 5, 100)  # Range for slope m\n",
    "b_range = np.linspace(-5, 5, 100)  # Range for intercept b\n",
    "\n",
    "# Meshgrid will make a 2D array \n",
    "M, B = np.meshgrid(m_range, b_range)\n",
    "\n",
    "# Calculate chi-squared for each combination of m and b\n",
    "chi_squared_values = np.zeros_like(M)\n",
    "\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        params = (M[i, j], B[i, j])\n",
    "        chi_squared_values[i, j] = linear_chi_square(params, dataset, theoretical_error_type='rmse')\n",
    "\n",
    "# Replotting the contour with the minimum point highlighted\n",
    "plt.figure(figsize=(10, 8))\n",
    "contour = plt.contourf(M, B, chi_squared_values, levels=50, cmap='viridis')\n",
    "plt.colorbar(contour, label=r'$\\chi^2$')\n",
    "\n",
    "# Highlight the minimum point\n",
    "plt.plot(optimized_parameters[0], optimized_parameters[1], marker='*', color='gold', markersize=15, label='Minimum $\\\\chi^2$')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(r'$\\chi^2$ Contour Plot with Minimum Highlighted', fontsize=16)\n",
    "plt.xlabel(r'Slope $m$', fontsize=14)\n",
    "plt.ylabel(r'Intercept $b$', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Considerations\n",
    "\n",
    "As we discussed above, there are additional tools we can use to make more helpful claims about the fit we just made. One of the first things we'll need is the Hessian Matrix which is used to determine if the convex function we optimized has a local minima.\n",
    "\n",
    "### Calculating the Hessian\n",
    "\n",
    "The Hessian can be calculated (as shown in [J Dobaczewski et al 2014 J. Phys. G: Nucl. Part. Phys. 41 074001](https://iopscience.iop.org/article/10.1088/0954-3899/41/7/074001)) by\n",
    "$$\n",
    "\\mathcal{M}_{\\alpha \\beta} = \\frac{1}{2} \\partial_\\alpha \\partial_\\beta \\chi^2 |_{\\vec{p}_0}\n",
    "$$\n",
    "where the partial derivatives are evaluated at the optimized parameter set $\\vec{p}_0$. In the case of our linear form, our equation is\n",
    "$$\n",
    "\\chi^2 = \\sum_i \\frac{\\left(mx_i + b - O_i^{\\rm exp}\\right)^2}{\\Delta O_i^2}\n",
    "$$\n",
    "for which we can generate the Hessian\n",
    "$$\n",
    "\\mathcal{M} = \\frac{1}{2} \\begin{bmatrix}\n",
    "\\frac{\\partial^2}{\\partial m^2}\\chi^2 & \\frac{\\partial^2}{\\partial m \\partial b}\\chi^2 \\\\\n",
    "\\frac{\\partial^2}{\\partial b \\partial m}\\chi^2 & \\frac{\\partial^2}{\\partial b^2}\\chi^2\n",
    "\\end{bmatrix} = \\frac{1}{2} \\begin{bmatrix}\n",
    "\\sum_i \\frac{2x_i^2}{\\Delta O_i^2} & \\sum_i \\frac{2x_i}{\\Delta O_i^2} \\\\\n",
    "\\sum_i \\frac{2x_i}{\\Delta O_i^2} & \\sum_i \\frac{2}{\\Delta O_i^2}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\sum_i \\frac{x_i^2}{\\Delta O_i^2} & \\sum_i \\frac{x_i}{\\Delta O_i^2} \\\\\n",
    "\\sum_i \\frac{x_i}{\\Delta O_i^2} & \\sum_i \\frac{1}{\\Delta O_i^2}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "This can be done analytically as shown and there are other packages like [```autograd```](https://github.com/HIPS/autograd/tree/7c22772cb42455c00dac964c5363242e62529ed6) which can do it for you automatically. We also show how one can do it with ```Sympy``` (a symbolic package) later in the \"Extras\" section.\n",
    "\n",
    "The important thing is that we first calculate our Hessian to then get the Covariance Matrix. We use the derived form above for the Hessian and implement the calculations via ```numpy``` array operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the RMSE for the theoretical error like we did in linear_chi_squared (or set to zero if you decided not to do this)\n",
    "rmse = np.sqrt(np.mean(\n",
    "    (dataset['y'] - linear_model(dataset['x'], optimized_parameters[0], optimized_parameters[1]))**2\n",
    ")) # Use RMSE to estimate theoretical error\n",
    "\n",
    "# Alternative comment the above rmse value and uncomment below to set to zero\n",
    "# rmse = 0\n",
    "\n",
    "# Create Hessian matrix first as a 2x2 array of zeros\n",
    "hessian_matrix = np.zeros((2,2), dtype=float)\n",
    "\n",
    "# Calculate first diagonal term i.e. partial d^2 / dm^2\n",
    "hessian_matrix[0,0] = np.sum(dataset['x']**2 / (dataset['error'] + rmse)**2)\n",
    "\n",
    "# Calculate the off-diagonals\n",
    "hessian_matrix[0,1] = np.sum(dataset['x'] / (dataset['error'] + rmse)**2)\n",
    "hessian_matrix[1,0] = np.sum(dataset['x'] / (dataset['error'] + rmse)**2)\n",
    "\n",
    "# Calculate the second diagonal i.e. partial d^2 / db^2\n",
    "hessian_matrix[1,1] = np.sum(1 / (dataset['error'] + rmse)**2)\n",
    "\n",
    "# Show our calculated Hessian\n",
    "print('Analytically calculated Hessian =')\n",
    "display(hessian_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Covariance Matrix\n",
    "\n",
    "Now that we have our Hessian $\\mathcal{M}$ we can calculate the covariance matrix $\\mathcal{C}$ via\n",
    "$$\n",
    "\\mathcal{C} = s \\mathcal{M}^{-1}\n",
    "$$\n",
    "where $s$ is\n",
    "$$\n",
    "s = \\frac{\\chi^2(\\vec{p}_0)}{N_d - N_p}.\n",
    "$$\n",
    "$N_d$ and $N_p$ are the number of observables and number of parameters respectively. Therefore, our analytical covariance matrix would be\n",
    "$$\n",
    "\\mathcal{M}^{-1} = \\frac{1}{\\left(\\sum_i \\frac{x_i^2}{\\Delta O_i^2}\\right) \\left(\\sum_i \\frac{1}{\\Delta O_i^2}\\right) - \\left(\\sum_i \\frac{x_i}{\\Delta O_i^2}\\right) \\left(\\sum_i \\frac{x_i}{\\Delta O_i^2}\\right)}\n",
    "\\begin{bmatrix}\n",
    "\\sum_i \\frac{1}{\\Delta O_i^2} & -\\sum_i \\frac{x_i}{\\Delta O_i^2} \\\\\n",
    "-\\sum_i \\frac{x_i}{\\Delta O_i^2} & \\sum_i \\frac{x_i^2}{\\Delta O_i^2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where we can just use the analytical form of a $2\\times 2$ inverse\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}^{-1} = \\frac{1}{ad - bc}\n",
    "\\begin{bmatrix}\n",
    "d & -b \\\\\n",
    "-c & a\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We calculate the inverse manually below using the analytical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the determinant of the Hessian (serves as our ad - bc value)\n",
    "hessian_det = np.linalg.det(hessian_matrix)\n",
    "\n",
    "# Create a zeros array to store the hessian inverse\n",
    "hessian_inv = np.zeros_like(hessian_matrix)\n",
    "\n",
    "# Set the values of the inverse according to the hessian locations\n",
    "hessian_inv[0,0] = hessian_matrix[1,1]\n",
    "hessian_inv[0,1] = -hessian_matrix[0,1]\n",
    "hessian_inv[1,0] = -hessian_matrix[1,0]\n",
    "hessian_inv[1,1] = hessian_matrix[0,0]\n",
    "\n",
    "# Scale by dividing by determinant\n",
    "hessian_inv = hessian_inv / hessian_det\n",
    "\n",
    "print('Analytical inverse of the Hessian =')\n",
    "display(hessian_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also calculate the inverse of a matrix numerically via ```numpy``` using the ```linalg.inv``` function. We note that ```linalg``` contains very helpful linear algebra functions which are fast and fairly user-friendly, so try to use these instead of coding your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numerical inverse of the Hessian =')\n",
    "display(np.linalg.inv(hessian_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the inverse of our Hessian, we can calculate our covariance matrix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate s or the normalized chi^2 using the optimal values\n",
    "covariance_s = normalized_linear_chi_square(optimized_parameters, dataset, 'RMSE')\n",
    "print('Value of s =', covariance_s)\n",
    "\n",
    "# Calculate covariance matrix\n",
    "covariance_matrix = covariance_s * hessian_inv\n",
    "\n",
    "print('Covariance matrix =')\n",
    "display(covariance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "\n",
    "Now that we have our covariance matrix, we can perform an analysis on the parameters of our model (our straight line). A correlation coefficient can inform us on how a quantity affects a model and highlight any underlying trends in its parameterization. In order to extract these helpful quantities, we must take our covariance matrix and divide by the variable standard deviation (or the diagonal of the covariance matrix). Ultimately we end up with\n",
    "$$\n",
    "c_{AB} = \\frac{|\\overline{\\Delta A \\Delta B}|}{\\sqrt{\\overline{\\Delta A^2}} \\sqrt{\\overline{\\Delta A^2}}}\n",
    "$$\n",
    "or if you prefer to consider it as a matrix of coefficients in our specific (linear case)\n",
    "$$\n",
    "c = \\begin{bmatrix}\n",
    "\\frac{|\\mathcal{C}_{mm}^2|}{\\sqrt{\\mathcal{C}_{mm}} \\sqrt{\\mathcal{C}_{mm}}} & \\frac{|\\mathcal{C}_{mm} \\mathcal{C}_{bb}|}{\\sqrt{\\mathcal{C}_{mm}} \\sqrt{\\mathcal{C}_{bb}}} \\\\\n",
    "\\frac{|\\mathcal{C}_{bb} \\mathcal{C}_{mm}|}{\\sqrt{\\mathcal{C}_{bb}} \\sqrt{\\mathcal{C}_{mm}}} & \\frac{|\\mathcal{C}_{bb}^2|}{\\sqrt{\\mathcal{C}_{bb}} \\sqrt{\\mathcal{C}_{bb}}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the standard deviations of our covariance matrix parameters along the diagonal\n",
    "standard_dev = np.sqrt(np.diag(covariance_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to divide by the standard deviations element-wise in the array, we can do a fancy trick! ```numpy```'s \"outer\" function will take two one-dimensional arrays and multiply them as an outer product to make a 2D array for us! For example, we can make the above denominator by doing the outer product\n",
    "$$\n",
    "\\xi = \\begin{bmatrix}\n",
    "\\sqrt{\\mathcal{C}_{mm}} \\\\\n",
    "\\sqrt{\\mathcal{C}_{bb}}\n",
    "\\end{bmatrix} \\quad {\\rm then} \\quad\n",
    "\\xi \\xi^T = \\begin{bmatrix}\n",
    "\\sqrt{\\mathcal{C}_{mm}} \\sqrt{\\mathcal{C}_{mm}} & \\sqrt{\\mathcal{C}_{mm}} \\sqrt{\\mathcal{C}_{bb}} \\\\\n",
    "\\sqrt{\\mathcal{C}_{bb}} \\sqrt{\\mathcal{C}_{mm}} & \\sqrt{\\mathcal{C}_{bb}} \\sqrt{\\mathcal{C}_{bb}}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "This can be implemented in ```numpy``` as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation coefficient matrix using an outer product\n",
    "correlation_coefficients = np.abs(covariance_matrix) / np.outer(standard_dev, standard_dev)\n",
    "\n",
    "print('Correlation coefficient matrix =')\n",
    "display(correlation_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two extremes of correlation coefficients are $c=1$ and $c=0$ where the parameters are completely correlated and fully independent respectively. We see in our case, the parameters on the diagonal are completely correlated. This is a great sanity check as we are considering cases where we are comparing a parameter to itself, so it should be 1! We see in the off-diagonals the correlation is still close to 1 so the $m$ and $b$ term also appear quite correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "Now that we've shown how to do $\\chi^2$ minimization for a line, we ask that you do the same process for the following function:\n",
    "$$\n",
    "y = ax^2 + bx + c\n",
    "$$\n",
    "Using the same dataset, please:\n",
    "1. Write a new python function for the above quadratic equation and a $\\chi^2$ function to be optimized (similar to how we did ```linear_model``` and ```linear_chi_square```) and minimize this function to get optimal parameters.\n",
    "\n",
    "2. Plot the optimized model on the same plot as the dataset. (Optional) Make a contour surface plot to highlight the minimum location.\n",
    "\n",
    "3. Calculate the Hessian and covariance matrices.\n",
    "\n",
    "4. Determine the correlation coefficients for this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own quadratic model\n",
    "\n",
    "# Define your own quadratic chi^2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize your chi^2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical analysis below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras\n",
    "\n",
    "## Using ```sympy```\n",
    "\n",
    "We show below how you can use ```sympy``` to perform the analytical calculations similar to how one might use other software like Mathematica. ```sympy``` can be convinient and has many helpful functions if other packages do not have them. That being said, it can be quite slow at times and should be used with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike other packages, sympy requires you to import things as needed like Matrices and variable names. These can always be modified afterwards, but you still need to import them like other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import ordered, Matrix\n",
    "from sympy import hessian as sympy_hessian\n",
    "\n",
    "# We import the symbols we will use in our equation\n",
    "from sympy.abc import m, b, p, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to keep things extra simple, we can just consider a case where we drop the sum in our $\\chi^2$ function and just look at one term\n",
    "$$\n",
    "\\frac{(mx_i + b - y_i)}{\\Delta O_i^2}.\n",
    "$$\n",
    "We use this in the following ```sympy``` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our chi^2 equation where y is the experimental data and p is a placeholder for our error\n",
    "eq = (m*x + b - y)**2 / p**2\n",
    "\n",
    "# Calling the equation name in Jupyter will print it in a nice LaTeX format\n",
    "eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [m, b] # Set the two variables we wish to differentiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a function to calculate the Jacobian which is used to calculate the Hessian\n",
    "gradient = lambda f, v: Matrix([f]).jacobian(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print our Jacobian\n",
    "gradient(eq, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then print the corresponding Sympy-calculated Hessian\n",
    "sympy_hessian(eq, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also try to calculate the inverse of the above Hessian\n",
    "sympy_hessian(eq, v).inv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that error doesn't appear good, so how do we reconcile the discrepancy between our analytical form and the ```sympy``` version? Thankfully, the answer is simple in that by only considering one term, we are actually not considering the correct matrix to invert! The presence of the zero-valued determinant error is a result of our matrix inversion equation from above as\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\frac{2x^2}{p^2} & \\frac{2x}{p^2} \\\\\n",
    "\\frac{2x}{p^2} & \\frac{2}{p^2}\n",
    "\\end{bmatrix}^{-1} = \\frac{1}{\\frac{2x^2}{p^2}\\frac{2}{p^2} - \\frac{2x}{p^2}\\frac{2x}{p^2}}\n",
    "\\begin{bmatrix}\n",
    "\\frac{2}{p^2} & -\\frac{2x}{p^2} \\\\\n",
    "-\\frac{2x}{p^2} & \\frac{2x^2}{p^2}\n",
    "\\end{bmatrix} = \\frac{1}{0}\n",
    "\\begin{bmatrix}\n",
    "\\frac{2}{p^2} & -\\frac{2x}{p^2} \\\\\n",
    "-\\frac{2x}{p^2} & \\frac{2x^2}{p^2}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "If we recall that our actual analytical case is the multiplication of sums, and consider order of operations, we see that\n",
    "$$\n",
    "\\left(\\sum_i \\frac{x_i^2}{\\Delta O_i^2}\\right) \\left(\\sum_i \\frac{1}{\\Delta O_i^2}\\right) \\neq \\left(\\sum_i \\frac{x_i}{\\Delta O_i^2}\\right) \\left(\\sum_i \\frac{x_i}{\\Delta O_i^2}\\right).\n",
    "$$\n",
    "This means that when we do ```sympy``` calculations, you should always remember that any tricks you do must be properly considered. We highlight this below by looping and printing over each matrix we would consider to make the Hessian properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will loop over each row of the dataset and call that set of values ('y', 'x', and 'error')\n",
    "for i in range(len(dataset)):\n",
    "    print(sympy_hessian(eq, v).subs(# For each call of the hessian function, we will then substitute in the proper values for the 'x' and 'p' variable\n",
    "        {x:dataset['x'].iloc[i], p:dataset['error'].iloc[i]+rmse}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = Matrix([[0,0],[0,0]])\n",
    "for i in range(len(dataset)):\n",
    "    temp += sympy_hessian(eq, v).subs(# For each call of the hessian function, we will then substitute in the proper values for the 'x' and 'p' variable\n",
    "            {x:dataset['x'].iloc[i], p:dataset['error'].iloc[i]+rmse}\n",
    "        )\n",
    "\n",
    "calculated_sympy_hessian = 0.5 * temp\n",
    "calculated_sympy_hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have properly summed over each term to construct the true Hessian, we can try to invert this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_sympy_hessian.inv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that this total Hessian can now be inverted properly and matches with our analytical case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERROR: Using ```autograd```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import hessian\n",
    "import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_hessian = hessian(linear_chi_square)\n",
    "\n",
    "optimized_hessian = linear_hessian(optimized_parameters, dataset, 'RMSE')\n",
    "\n",
    "print('Autograd Hessian')\n",
    "display(optimized_hessian)\n",
    "display(np.linalg.inv(optimized_hessian))\n",
    "\n",
    "print('Analytic Hessian')\n",
    "display(hessian_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
